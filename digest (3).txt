# Repository Analysis

## Summary

Directory: home/artem.d/Projects/Python/tabletopmagnat/src
Files analyzed: 58

Estimated tokens: 15.9k

## Directory Structure

Directory structure:
└── src/
    └── tabletopmagnat/
        ├── __init__.py
        ├── application/
        │   ├── __init__.py
        │   └── application.py
        ├── config/
        │   ├── __init__.py
        │   ├── config.py
        │   ├── langfuse.py
        │   ├── mcp_tools.py
        │   ├── models.py
        │   └── openai_config.py
        ├── node/
        │   ├── __init__.py
        │   ├── abstract_node.py
        │   ├── abstract_parallel_node.py
        │   ├── assistant_node.py
        │   ├── chonkie_node.py
        │   ├── debug_node.py
        │   ├── docling_node.py
        │   ├── echo_node.py
        │   ├── empty_user_node.py
        │   ├── expert_parallel_coordinator_node.py
        │   ├── from_summary_to_main_node.py
        │   ├── join_node.py
        │   ├── llm_node.py
        │   ├── mcp_tool_node.py
        │   ├── security_llm_node.py
        │   ├── summary_node.py
        │   ├── task_classifier_node.py
        │   └── task_splitter_node.py
        ├── pocketflow/
        │   ├── __init__.py
        │   └── __init__.pyi.py
        ├── services/
        │   ├── __init__.py
        │   └── openai_service.py
        ├── state/
        │   ├── __init__.py
        │   ├── expert_state.py
        │   └── private_state.py
        ├── structured_output/
        │   ├── __init__.py
        │   ├── security.py
        │   ├── task_classifier.py
        │   └── task_splitter.py
        ├── subgraphs/
        │   ├── __init__.py
        │   └── rags.py
        └── types/
            ├── __init__.py
            ├── dialog/
            │   ├── __init__.py
            │   └── dialog.py
            ├── messages/
            │   ├── __init__.py
            │   ├── ai_messages.py
            │   ├── base_message.py
            │   ├── developer_messages.py
            │   ├── message_roles.py
            │   ├── system_messages.py
            │   ├── tool_message.py
            │   └── user_messages.py
            └── tool/
                ├── __init__.py
                ├── openai_tool_params.py
                ├── tool_header.py
                └── mcp/
                    ├── __init__.py
                    ├── mcp_server.py
                    ├── mcp_servers.py
                    └── mcp_tool.py


## Files Content

================================================
FILE: tabletopmagnat/__init__.py
================================================
[Empty file]


================================================
FILE: tabletopmagnat/application/__init__.py
================================================
[Empty file]


================================================
FILE: tabletopmagnat/application/application.py
================================================
from uuid import uuid4

from langfuse import Langfuse

from tabletopmagnat.config.config import Config
from tabletopmagnat.node.echo_node import EchoNode
from tabletopmagnat.node.expert_parallel_coordinator_node import (
    ExpertParallelCoordinator,
)
from tabletopmagnat.node.from_summary_to_main_node import FromSummaryToMain
from tabletopmagnat.node.join_node import JoinNode
from tabletopmagnat.node.llm_node import LLMNode
from tabletopmagnat.node.security_llm_node import SecurityNode
from tabletopmagnat.node.task_classifier_node import TaskClassifierNode
from tabletopmagnat.node.task_splitter_node import TaskSplitterNode
from tabletopmagnat.pocketflow import AsyncFlow
from tabletopmagnat.services.openai_service import OpenAIService
from tabletopmagnat.state.expert_state import ExpertState
from tabletopmagnat.state.private_state import PrivateState
from tabletopmagnat.structured_output.security import SecurityOutput
from tabletopmagnat.structured_output.task_classifier import TaskClassifierOutput
from tabletopmagnat.structured_output.task_splitter import TaskSplitterOutput
from tabletopmagnat.subgraphs.rags import RASG
from tabletopmagnat.types.messages import UserMessage
from tabletopmagnat.types.tool import ToolHeader
from tabletopmagnat.types.tool.mcp import MCPServer, MCPServers, MCPTools


class Application:
    """Main application class that orchestrates nodes and tools to process user input using LLM and external APIs.

    Attributes:
        config (Config): Configuration object loaded from the application's config module.
        langfuse (Langfuse): Langfuse client for observability and tracing.
        _llm (OpenAIService): Language model service used for message generation.
        task_classifier (TaskClassifierNode | None): Node responsible for classifying tasks based on input.
        tool_node (MCPToolNode | None): Node responsible for invoking external tools.
        debug_node (DebugNode | None): Final node for logging or debugging output.
        flow (AsyncFlow | None): Asynchronous workflow composed of connected nodes.
        shared_data (dict[str, Any]): Shared context between nodes, containing the dialog history.
    """

    def __init__(self) -> None:
        """Initialize the Application with configuration, services, and placeholder nodes."""
        self.config = Config()
        self.langfuse = Langfuse(
            host=self.config.langfuse.host,
            public_key=self.config.langfuse.public_key,
            secret_key=self.config.langfuse.secret_key,
        )

        # Service

        self.general_llm = OpenAIService(
            self.config.models.general_model, self.config.openai
        )
        # ---
        self.task_splitter_llm = OpenAIService(
            self.config.models.general_model, self.config.openai
        )
        self.task_splitter_llm.bind_structured(TaskSplitterOutput)
        # ---
        self.task_classifier_llm = OpenAIService(
            self.config.models.general_model, self.config.openai
        )
        self.task_classifier_llm.bind_structured(TaskClassifierOutput)
        # ---
        self.security_llm = OpenAIService(
            self.config.models.security_model, self.config.openai
        )
        self.security_llm.bind_structured(SecurityOutput)
        # ---
        # TODO: FIX TYPOS
        self.rasg_llm = OpenAIService(self.config.models.rags_model, self.config.openai)

        # Nodes
        self.security_node: SecurityNode | None = None
        self.echo_node: EchoNode | None = None
        self.task_classifier_node: TaskClassifierNode | None = None
        self.task_splitter_node: TaskSplitterNode | None = None
        self.expert_parallel_coordinator: ExpertParallelCoordinator | None = None
        self.join_node: JoinNode | None = None
        self.summary_node: LLMNode | None = None
        self.swicth_node: FromSummaryToMain | None = None

        self.expert_1: AsyncFlow | None = None
        self.expert_2: AsyncFlow | None = None
        self.expert_3: AsyncFlow | None = None

        # Flow
        self.flow: AsyncFlow | None = None

        # Data
        self.shared_data = PrivateState()

    async def init_nodes(self):
        """Initialize application nodes if they have not been created yet.

        This method lazily initializes the `task_classifier`, `tool_node`, and `final_node` by calling their respective
        factory methods. Each node is initialized only once.
        """
        self.security_node = SecurityNode(
            name="security",
            llm_service=self.security_llm,
            prompt_name="security",
            dialog_selector=lambda x: x.dialog,
        )

        self.echo_node = EchoNode(name="echo", echo_text="Sorry, but I can't help you.")

        self.task_splitter_node = TaskSplitterNode(
            name="task_splitter",
            llm_service=self.task_splitter_llm,
            prompt_name="task_splitter",
            dialog_selector=lambda x: x.dialog,
        )

        self.task_classifier_node = TaskClassifierNode(
            name="task_classifier",
            llm_service=self.task_classifier_llm,
            prompt_name="task_classifier",
            dialog_selector=lambda x: x.dialog,
        )

        tools = self.get_tools()
        _ = await tools.get_tool_list()
        self.expert_1 = await RASG.create_subgraph(
            name="expert_1",
            prompt_name="expert_1",
            openai_service=self.rasg_llm,
            mcp_tools=tools,
            dialog_selector=lambda x: x.expert_1,
        )

        self.expert_2 = await RASG.create_subgraph(
            name="expert_2",
            prompt_name="expert_2",
            openai_service=self.rasg_llm,
            mcp_tools=tools,
            dialog_selector=lambda x: x.expert_2,
        )

        self.expert_3 = await RASG.create_subgraph(
            name="expert_3",
            prompt_name="expert_3",
            openai_service=self.rasg_llm,
            mcp_tools=tools,
            dialog_selector=lambda x: x.expert_3,
        )

        self.expert_parallel_coordinator = ExpertParallelCoordinator(
            name="expert_parallel_coordinator",
            expert_state=ExpertState(
                expert_1=self.expert_1, expert_2=self.expert_2, expert_3=self.expert_3
            ),
        )

        self.join_node = JoinNode(name="join")

        self.summary_node = LLMNode(
            name="summary",
            prompt_name="summary",
            llm_service=self.general_llm,
            dialog_selector=lambda x: x.summary,
        )

        self.swicth_node = FromSummaryToMain(name="switch")

    def get_tools(self):
        """Construct and return a set of external tools (e.g., MCP API).

        Returns:
            MCPTools: A collection of external tools configured for use in the application.
        """
        header = ToolHeader(Authorization="Bearer 1234567890")
        server = MCPServer(
            transport="http",
            url="http://localhost:8000/mcp",
            headers=header,
            auth="bearer",
        )
        mcp_servers = MCPServers(mcpServers={"mcp": server})
        return MCPTools(mcp_servers)

    def connect_nodes(self):
        """Connect the nodes into a workflow.

        This method defines the data flow between the task classifier, tool node, and final debug node.
        """
        self.security_node - "unsafe" >> self.echo_node
        self.security_node - "safe" >> self.task_classifier_node

        self.task_classifier_node - "explanation" >> self.task_splitter_node
        self.task_splitter_node >> self.expert_parallel_coordinator
        self.expert_parallel_coordinator >> self.join_node
        self.join_node >> self.summary_node
        self.summary_node >> self.swicth_node

    async def init_flow(self):
        """Initialize the workflow by setting up nodes and connecting them.

        This method ensures all necessary nodes are initialized and then connects them into an `AsyncFlow`.
        """
        await self.init_nodes()
        self.connect_nodes()
        self.flow = AsyncFlow(start=self.security_node)

    async def run(self, msg=""):
        """Run the application workflow with a sample user message.

        This method adds a user message to the dialog, initializes the workflow if it hasn't been created yet,
        and executes the workflow asynchronously. It also logs the input and output using Langfuse.
        """

        self.shared_data.dialog.add_message(UserMessage(content=msg))

        if self.flow is None:
            await self.init_flow()

        with self.langfuse.start_as_current_span(name=f"Span:{uuid4()}") as span:
            span.update(input=self.shared_data.dialog)

            await self.flow.run_async(shared=self.shared_data)

            last_msg = self.shared_data.dialog.get_last_message()
            span.update(output=last_msg)



================================================
FILE: tabletopmagnat/config/__init__.py
================================================
[Empty file]


================================================
FILE: tabletopmagnat/config/config.py
================================================
"""
Application Configuration Module.

This module provides the main configuration class for the TabletopMagnat application.
It uses `pydantic_settings` to load configuration values from environment variables
and a `.env` file, allowing for structured and type-safe access to configuration data.

Key Features:
- Automatic loading of environment variables from `.env`.
- Support for nested configuration via `env_nested_delimiter`.
- Integration with service-specific configurations (e.g., OpenAI).

Classes:
    Config: Main configuration class that includes service-specific configurations.
"""

from pydantic_settings import BaseSettings, SettingsConfigDict  # type: ignore

from tabletopmagnat.config.langfuse import LangfuseSettings
from tabletopmagnat.config.models import Models
from tabletopmagnat.config.openai_config import OpenAIConfig


class Config(BaseSettings):
    """
    Main application configuration class.

    Uses `pydantic_settings.BaseSettings` to automatically load settings from
    environment variables and the `.env` file.

    Attributes:
        model_config (SettingsConfigDict): Pydantic configuration specifying
            the `.env` file path, encoding, and nested parameter delimiter.
        openai (OpenAIConfig): Nested configuration for OpenAI services.
    """

    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        env_nested_delimiter="__"
    )
    models: Models = Models()
    openai: OpenAIConfig = OpenAIConfig()
    langfuse: LangfuseSettings = LangfuseSettings()


================================================
FILE: tabletopmagnat/config/langfuse.py
================================================
from pydantic_settings import BaseSettings


class LangfuseSettings(BaseSettings):
    public_key: str = ""
    secret_key: str = ""
    host: str = ""


================================================
FILE: tabletopmagnat/config/mcp_tools.py
================================================
from pydantic_settings import BaseSettings


class MCPSettings(BaseSettings):
    mcp_objectbox_url: str



================================================
FILE: tabletopmagnat/config/models.py
================================================
from pydantic_settings import BaseSettings

class Models(BaseSettings):
    security_model: str = "gpt-oss:latest"
    general_model: str = "gpt-oss:latest"
    rags_model: str = "gpt-oss:latest"


================================================
FILE: tabletopmagnat/config/openai_config.py
================================================
"""
OpenAI Configuration Module.

This module defines the configuration class for OpenAI services used in the TabletopMagnat application.
It provides a structured way to manage and access OpenAI-specific settings such as API key, base URL,
and model name using Pydantic's `BaseSettings`.

Classes:
    OpenAIConfig: Represents the configuration for OpenAI services.
"""

from pydantic_settings import BaseSettings  # type: ignore 


class OpenAIConfig(BaseSettings):
    """
    Configuration class for OpenAI services.

    Attributes:
        api_key (str): The API key used to authenticate with OpenAI services.
        base_url (str): The base URL of the OpenAI API endpoint.
    """

    api_key: str = ""
    base_url: str = ""



================================================
FILE: tabletopmagnat/node/__init__.py
================================================
[Empty file]


================================================
FILE: tabletopmagnat/node/abstract_node.py
================================================
from abc import ABC

from langfuse import get_client
from tabletopmagnat.pocketflow import AsyncNode


class AbstractNode(AsyncNode, ABC):
    def __init__(self, name: str, max_retries=3, wait: int | float = 1):
        super().__init__(max_retries=max_retries, wait=wait)
        self._name = name
        self._lf_client = get_client()



================================================
FILE: tabletopmagnat/node/abstract_parallel_node.py
================================================
from abc import ABC

from langfuse import get_client
from tabletopmagnat.pocketflow import AsyncParallelBatchNode


class AbstractParallelNode(AsyncParallelBatchNode, ABC):
    def __init__(self, name: str, max_retries=10, wait: int | float = 10):
        super().__init__(max_retries=max_retries, wait=wait)
        self._name = name
        self._lf_client = get_client()



================================================
FILE: tabletopmagnat/node/assistant_node.py
================================================
from typing import override

from langfuse import observe

from tabletopmagnat.node.llm_node import LLMNode
from tabletopmagnat.types.messages import SystemMessage


class AssistantNode(LLMNode):

    @override
    @observe(as_type="chain")
    def get_prompt(self) -> SystemMessage:
        name = f"{self._name}:get_prompt"
        self._lf_client.update_current_span(name=name)

        prompt = self._lf_client.get_prompt("main")
        sys_msg = SystemMessage(content=prompt.prompt)
        return sys_msg



================================================
FILE: tabletopmagnat/node/chonkie_node.py
================================================
"""

"""
from chonkie import MarkdownChef, MarkdownDocument

from tabletopmagnat.node.abstract_node import AbstractNode


class ChonkieNode(AbstractNode):
    def __init__(self, name: str, max_retries=10, wait: int | float = 10):
        super().__init__(name, max_retries, wait)
        self._chef: MarkdownChef = MarkdownChef()

    async def prep_async(self, shared):
        doc = shared["document"]
        return doc

    async def exec_async(self, prep_res):
        document = self._chef.parse(prep_res)
        return document

    async def post_async(self, shared, prep_res, exec_res):
        document: MarkdownDocument = exec_res
        original_document = shared["document"]

        for chunk in document.chunks:
            chunk.text = original_document["title"]

        shared["chunks"] = exec_res
        return "default"



================================================
FILE: tabletopmagnat/node/debug_node.py
================================================
from icecream import ic
from pocketflow import AsyncNode


class DebugNode(AsyncNode):
    async def prep_async(self, shared):
        ic("DebugNode:prep_async|", shared)
        return shared

    async def run_async(self, shared):
        ic("DebugNode:run_async|", shared)
        return None

    async def post_async(self, shared, prep_res, exec_res):
        ic("DebugNode:post_async|", shared, prep_res, exec_res)
        return "default"



================================================
FILE: tabletopmagnat/node/docling_node.py
================================================
from typing import override

from docling.datamodel import vlm_model_specs
from docling.datamodel.accelerator_options import AcceleratorOptions
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import VlmPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.pipeline.vlm_pipeline import VlmPipeline
from langfuse import observe

from tabletopmagnat.node.abstract_node import AbstractNode


class DoclingNode(AbstractNode):
    def __init__(self, name: str, max_retries=1, wait: int | float = 10):
        super().__init__(name, max_retries, wait)
        pipeline_options = VlmPipelineOptions(
            vlm_options=vlm_model_specs.GRANITE_VISION_VLLM,
            accelerator_options=AcceleratorOptions(
                device="AUTO",
                num_threads=10,

            )
        )
        self._converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(
                    pipeline_cls=VlmPipeline,
                    pipeline_options=pipeline_options,
                ),
            }
        )

    @override
    @observe
    async def prep_async(self, shared):
        name = f"{self._name}:prep"
        self._lf_client.update_current_span(name=name)

        dialog = shared["dialog"]
        last_msg = dialog.get_last_message()
        url = last_msg.content
        return url

    @override
    @observe
    async def exec_async(self, prep_res):
        name = f"{self._name}:exec"
        self._lf_client.update_current_span(name=name)

        converted = self._converter.convert(prep_res)
        doc = converted.document
        data = {
            "document": doc,
            "title": doc.name.title(),
            "md": doc.export_to_markdown(),
        }
        return data

    @override
    @observe
    async def post_async(self, shared, prep_res, exec_res):
        name = f"{self._name}:post"
        self._lf_client.update_current_span(name=name)

        shared["document"] = exec_res
        return "default"



================================================
FILE: tabletopmagnat/node/echo_node.py
================================================
from typing import override

from tabletopmagnat.node.abstract_node import AbstractNode
from tabletopmagnat.state.private_state import PrivateState
from tabletopmagnat.types.dialog import Dialog
from tabletopmagnat.types.messages import AiMessage


class EchoNode(AbstractNode):
    def __init__(self, name: str, echo_text: str, max_retries=10, wait: int | float = 10):
        super().__init__(name, max_retries, wait)
        self._echo_text = echo_text

    async def prep_async(self, shared: PrivateState):
        return None

    async def exec_async(self, prep_res):
        return AiMessage(content=self._echo_text)

    async def post_async(self, shared: PrivateState, prep_res, exec_res):
        shared.dialog.add_message(exec_res)
        return "default"



================================================
FILE: tabletopmagnat/node/empty_user_node.py
================================================
from pocketflow import AsyncNode
from tabletopmagnat.types.dialog import Dialog

from tabletopmagnat.types.messages import UserMessage


class EmptyUserNode(AsyncNode):
    async def exec_async(self, prep_res):
        return UserMessage(content="")

    async def post_async(self, shared, prep_res, exec_res):
        dialog: Dialog = shared["dialog"]
        dialog.add_message(exec_res)
        return "default"



================================================
FILE: tabletopmagnat/node/expert_parallel_coordinator_node.py
================================================
from typing import override

from langfuse import observe

from tabletopmagnat.node.abstract_parallel_node import AbstractParallelNode
from tabletopmagnat.pocketflow import AsyncFlow
from tabletopmagnat.state.expert_state import ExpertState
from tabletopmagnat.state.private_state import PrivateState


class ExpertParallelCoordinator(AbstractParallelNode):
    def __init__(
        self,
        name,
        expert_state: ExpertState,
        max_retries=3,
        wait: int | float = 1,
    ):
        super().__init__(name, max_retries, wait)
        self._expert_state = expert_state

    @observe(as_type="chain")
    @override
    async def prep_async(
        self, shared: PrivateState
    ) -> list[tuple[AsyncFlow, PrivateState]]:
        res = [(expert, shared) for expert in self._expert_state.to_list()]
        return res

    @observe(as_type="chain")
    @override
    async def exec_async(self, prep_res: tuple[AsyncFlow, PrivateState]) -> None:
        flow, shared = prep_res
        await flow.run_async(shared)

    @observe(as_type="chain")
    @override
    async def post_async(
        self,
        shared: PrivateState,
        prep_res: tuple[AsyncFlow, PrivateState],
        exec_res: None,
    ) -> str:
        return "default"



================================================
FILE: tabletopmagnat/node/from_summary_to_main_node.py
================================================
from typing import override

from tabletopmagnat.node.abstract_node import AbstractNode
from tabletopmagnat.state.private_state import PrivateState
from tabletopmagnat.types.dialog import Dialog
from tabletopmagnat.types.messages import AiMessage


class FromSummaryToMain(AbstractNode):
    @override
    async def prep_async(self, shared: PrivateState):
        return shared.summary

    @override
    async def exec_async(self,prep_res: Dialog):
        last_msg = prep_res.get_last_message()
        return last_msg

    @override
    async def post_async(self,shared: PrivateState,prep_res:Dialog,exec_res:AiMessage):
        shared.dialog.add_message(exec_res)
        return "default"


================================================
FILE: tabletopmagnat/node/join_node.py
================================================
from typing import Literal, override

from langfuse import observe

from tabletopmagnat.state.private_state import PrivateState
from tabletopmagnat.types.dialog import Dialog
from tabletopmagnat.types.messages import UserMessage
from tabletopmagnat.node.abstract_node import AbstractNode

class JoinNode(AbstractNode):
    @observe(as_type="chain")
    @override
    async def prep_async(self, shared: PrivateState) -> list[Dialog]:
        experts_dialog = [shared.expert_1, shared.expert_2, shared.expert_3]

        return experts_dialog

    @observe(as_type="chain")
    @override
    async def exec_async(self, prep_res: list[Dialog]) -> str:
        experts_dialog = prep_res
        experts_dialog = [
            dialog.get_last_message().content for dialog in experts_dialog
        ]
        result = "\n---\n".join(experts_dialog)

        return result

    @observe(as_type="chain")
    @override
    async def post_async(
        self,
        shared: PrivateState,
        prep_res: list[Dialog],
        exec_res: str,
    ) -> Literal["default"]:
        summery: Dialog = shared.summary
        message = f"Create a summary of the following rulebook created by experts:\n{exec_res}"
        summery.add_message(UserMessage(content=message))
        return "default"



================================================
FILE: tabletopmagnat/node/llm_node.py
================================================
from abc import abstractmethod
from typing import Any, Callable

from icecream import ic
from langfuse import observe

from tabletopmagnat.node.abstract_node import AbstractNode
from tabletopmagnat.services.openai_service import OpenAIService
from tabletopmagnat.state.private_state import PrivateState
from tabletopmagnat.types.dialog import Dialog
from tabletopmagnat.types.messages import AiMessage, SystemMessage
from tabletopmagnat.types.tool.openai_tool_params import OpenAIToolParams


class LLMNode(AbstractNode):
    def __init__(
        self,
        name: str,
        prompt_name: str,
        dialog_selector: Callable[[Any], Dialog],
        llm_service: OpenAIService,
        max_retries=10,
        wait: float = 10,
    ):
        super().__init__(name, max_retries=max_retries, wait=wait)
        self._prompt_name = prompt_name
        self._llm = llm_service
        self._dialog_selector = dialog_selector

    def bind_tools(self, tools: list[OpenAIToolParams]):
        self._llm.add_mcp_tools(tools)

    def get_prompt(self) -> SystemMessage:
        name = f"{self._name}:get_prompt"
        self._lf_client.update_current_span(name=name)

        prompt = self._lf_client.get_prompt(self._prompt_name)
        return SystemMessage(content=prompt.prompt)

    # ---------- PREP ----------
    @observe(as_type="chain")
    async def prep_async(self, shared: PrivateState):
        name = f"{self._name}:prep"
        self._lf_client.update_current_span(name=name)
        dialog = self._dialog_selector(shared)
        return dialog

    # ---------- EXEC ----------
    @observe(as_type="generation")
    async def exec_async(self, prepared_prep: Dialog) -> AiMessage:
        name = f"{self._name}:exec"
        self._lf_client.update_current_generation(name=name)

        dialog = Dialog(messages=[self.get_prompt()])
        dialog += prepared_prep
        result: AiMessage = await self._llm.generate(dialog)

        return result

    # ---------- POST ----------
    @observe(as_type="chain")
    async def post_async(self, shared: PrivateState, prep_res: Dialog, exec_res: AiMessage):
        name = f"{self._name}:post"
        self._lf_client.update_current_span(name=name)

        ic("LLMNode:post | exec_res:", exec_res)

        prep_res.add_message(exec_res)
        if exec_res.tool_calls:
            return "tools"
        return "default"



================================================
FILE: tabletopmagnat/node/mcp_tool_node.py
================================================
import json
from typing import Callable

from icecream import ic
from langfuse import observe

from tabletopmagnat.node.abstract_node import AbstractNode
from tabletopmagnat.state.private_state import PrivateState
from tabletopmagnat.types.dialog import Dialog
from tabletopmagnat.types.messages import AiMessage
from tabletopmagnat.types.messages.tool_message import ToolMessage
from tabletopmagnat.types.tool.mcp import MCPTools


class MCPToolNode(AbstractNode):
    def __init__(
        self,
        name: str,
        dialog_selector: Callable[[PrivateState], Dialog],
        mcp_tool: MCPTools,
        max_retires: int = 1,
        wait: float = 0,

    ):
        super().__init__(name, max_retires, wait)
        self._mcp_tool = mcp_tool
        self._dialog_selector = dialog_selector

    @observe(as_type="tool")
    async def prep_async(self, shared):
        name = f"{self._name}:prep"
        self._lf_client.update_current_span(name=name)

        dialog = self._dialog_selector(shared)
        last_msg: AiMessage = dialog.get_last_message()

        return last_msg.internal_tools or []

    @observe(as_type="tool")
    async def exec_async(self, prep_res):
        name = f"{self._name}:exec"
        self._lf_client.update_current_span(name=name)

        tool_calls: list[ToolMessage] = prep_res
        for tool_call in tool_calls:
            res = await self._mcp_tool.call_tool(tool_call.name, tool_call.content)
            tool_call.content = json.dumps(res.structured_content or "")
            ic("ToolNode:exec_async | tool result:", res)
        ic("ToolNode:exec_async | all tool calls:", tool_calls)
        return tool_calls

    @observe(as_type="tool")
    async def post_async(self, shared:PrivateState, prep_res, exec_res: list[ToolMessage]):
        name = f"{self._name}:post"
        self._lf_client.update_current_span(name=name)

        tool_calls: list[ToolMessage] = exec_res
        dialog = self._dialog_selector(shared)

        for tool_call in tool_calls:
            dialog.add_message(tool_call)

        return "default"



================================================
FILE: tabletopmagnat/node/security_llm_node.py
================================================
from typing import override

from icecream import ic
from langfuse import observe

from tabletopmagnat.node.llm_node import LLMNode
from tabletopmagnat.state.private_state import PrivateState
from tabletopmagnat.types.dialog import Dialog
from tabletopmagnat.types.messages import SystemMessage, AiMessage


class SecurityNode(LLMNode):
    @override
    @observe(as_type="guardrail")
    def get_prompt(self) -> SystemMessage:
        name = f"{self._name}:get_prompt"
        self._lf_client.update_current_span(name=name)

        prompt = self._lf_client.get_prompt("security")
        return SystemMessage(content=prompt.prompt)

    @override
    @observe(name="TaskClassifier:post", as_type="chain")
    async def post_async(self, shared: PrivateState, prep_res, exec_res):
        """Handles post-processing after execution.

        Logs the AI response using `icecream`, adds the AI message to the dialog, and returns a status.

        Args:
            shared (dict[str, Any]): Shared context containing the dialog.
            prep_res (Dialog): The prepared dialog (not used here).
            exec_res (AiMessage): The AI-generated message from execution.

        Returns:
            str: A default return value ("default") indicating completion.
        """
        self._lf_client.update_current_span(name=f"{self._name}:post", metadata=exec_res.metadata)

        ic("TaskClassifier:post| exec_res:", exec_res)
        msg: AiMessage = exec_res

        assert "verdict" in msg.metadata, "No verdict in metadata"

        return msg.metadata["verdict"]


================================================
FILE: tabletopmagnat/node/summary_node.py
================================================
from langfuse import observe

from tabletopmagnat.node.abstract_node import AbstractNode
from tabletopmagnat.types.dialog import Dialog
from tabletopmagnat.types.messages import MessageRoles


class SummaryNode(AbstractNode):
    @observe(as_type="chain")
    async def prep_async(self, shared):
        name = f"{self._name}:prep"
        self._lf_client.update_current_span(name=name)

        dialog: Dialog = shared["dialog"]

        if len(dialog.messages) < 2:
            return shared

        last_msg = dialog.pop_last_message()
        if last_msg.role != MessageRoles.ASSISTANT:
            dialog.add_message(last_msg)
            return shared

        content_suffix = (
            f"\n\n<END_AGENT_TURN>\n\n---\n\n<ANSWER_DIFFERENT_AGENT>\n{last_msg.content}"
        )

        prev_msg = dialog.pop_last_message()
        if prev_msg.role != MessageRoles.USER:
            dialog.add_message(prev_msg)
            dialog.add_message(last_msg)
            raise RuntimeError(
                "SummaryNode: Broken dialog structure — expected USER before ASSISTANT"
            )

        prev_msg.content = (prev_msg.content or "") + content_suffix
        dialog.add_message(prev_msg)
        return shared



================================================
FILE: tabletopmagnat/node/task_classifier_node.py
================================================
from typing import override

from icecream import ic
from langfuse import observe

from tabletopmagnat.node.llm_node import LLMNode
from tabletopmagnat.state.private_state import PrivateState
from tabletopmagnat.types.dialog import Dialog
from tabletopmagnat.types.messages import SystemMessage, AiMessage


class TaskClassifierNode(LLMNode):
    @override
    @observe(name="TaskClassifier:get_prompt")
    def get_prompt(self) -> SystemMessage:
        self._lf_client.update_current_span(name=f"{self._name}:get_prompt")
        prompt = self._lf_client.get_prompt("task_classifier")
        return SystemMessage(content=prompt.prompt)

    @override
    @observe(name="TaskClassifier:post", as_type="chain")
    async def post_async(self, shared: PrivateState, prep_res, exec_res):
        """Handles post-processing after execution.

        Logs the AI response using `icecream`, adds the AI message to the dialog, and returns a status.

        Args:
            shared (dict[str, Any]): Shared context containing the dialog.
            prep_res (Dialog): The prepared dialog (not used here).
            exec_res (AiMessage): The AI-generated message from execution.

        Returns:
            str: A default return value ("default") indicating completion.
        """
        self._lf_client.update_current_span(name=f"{self._name}:post")

        ic("TaskClassifier:post| exec_res:", exec_res)
        msg: AiMessage = exec_res

        assert "task" in msg.metadata, "No task in metadata"
        task = msg.metadata["task"]
        return task


================================================
FILE: tabletopmagnat/node/task_splitter_node.py
================================================
from typing import override

from langfuse import observe

from tabletopmagnat.node.llm_node import LLMNode
from tabletopmagnat.state.private_state import PrivateState
from tabletopmagnat.types.messages import AiMessage, SystemMessage, UserMessage


class TaskSplitterNode(LLMNode):
    @override
    @observe(as_type="chain")
    def get_prompt(self) -> SystemMessage:
        name = f"{self._name}:get_prompt"
        self._lf_client.update_current_span(name=name)

        prompt = self._lf_client.get_prompt("task_splitter")
        return SystemMessage(content=prompt.prompt)


    def prepare_message(self, content: str) -> str:
        msg = f"Here is your task: {content}"
        msg += "\n---\n Always use tools to get information for task."
        return msg

    @override
    async def post_async(self, shared: PrivateState, prep_res, exec_res: AiMessage):
        assert "task_for_expert1" in exec_res.metadata, "No task for expert1 found"
        msg = self.prepare_message(exec_res.metadata["task_for_expert1"])
        shared.expert_1.add_message(UserMessage(content=msg))

        assert "task_for_expert2" in exec_res.metadata, "No task for expert2 found"
        msg = self.prepare_message(exec_res.metadata["task_for_expert2"])
        shared.expert_2.add_message(UserMessage(content=msg))

        assert "task_for_expert3" in exec_res.metadata, "No task for expert3 found"
        msg = self.prepare_message(exec_res.metadata["task_for_expert3"])
        shared.expert_3.add_message(UserMessage(content=msg))

        return "default"



================================================
FILE: tabletopmagnat/pocketflow/__init__.py
================================================
import asyncio, warnings, copy, time

class BaseNode:
    def __init__(self): self.params,self.successors={},{}
    def set_params(self,params): self.params=params
    def next(self,node,action="default"):
        if action in self.successors: warnings.warn(f"Overwriting successor for action '{action}'")
        self.successors[action]=node; return node
    def prep(self,shared): pass
    def exec(self,prep_res): pass
    def post(self,shared,prep_res,exec_res): pass
    def _exec(self,prep_res): return self.exec(prep_res)
    def _run(self,shared): p=self.prep(shared); e=self._exec(p); return self.post(shared,p,e)
    def run(self,shared):
        if self.successors: warnings.warn("Node won't run successors. Use Flow.")
        return self._run(shared)
    def __rshift__(self,other): return self.next(other)
    def __sub__(self,action):
        if isinstance(action,str): return _ConditionalTransition(self,action)
        raise TypeError("Action must be a string")

class _ConditionalTransition:
    def __init__(self,src,action): self.src,self.action=src,action
    def __rshift__(self,tgt): return self.src.next(tgt,self.action)

class Node(BaseNode):
    def __init__(self,max_retries=1,wait=0): super().__init__(); self.max_retries,self.wait=max_retries,wait
    def exec_fallback(self,prep_res,exc): raise exc
    def _exec(self,prep_res):
        for self.cur_retry in range(self.max_retries):
            try: return self.exec(prep_res)
            except Exception as e:
                if self.cur_retry==self.max_retries-1: return self.exec_fallback(prep_res,e)
                if self.wait>0: time.sleep(self.wait)

class BatchNode(Node):
    def _exec(self,items): return [super(BatchNode,self)._exec(i) for i in (items or [])]

class Flow(BaseNode):
    def __init__(self,start=None): super().__init__(); self.start_node=start
    def start(self,start): self.start_node=start; return start
    def get_next_node(self,curr,action):
        nxt=curr.successors.get(action or "default")
        if not nxt and curr.successors: warnings.warn(f"Flow ends: '{action}' not found in {list(curr.successors)}")
        return nxt
    def _orch(self,shared,params=None):
        curr,p,last_action =copy.copy(self.start_node),(params or {**self.params}),None
        while curr: curr.set_params(p); last_action=curr._run(shared); curr=copy.copy(self.get_next_node(curr,last_action))
        return last_action
    def _run(self,shared): p=self.prep(shared); o=self._orch(shared); return self.post(shared,p,o)
    def post(self,shared,prep_res,exec_res): return exec_res

class BatchFlow(Flow):
    def _run(self,shared):
        pr=self.prep(shared) or []
        for bp in pr: self._orch(shared,{**self.params,**bp})
        return self.post(shared,pr,None)

class AsyncNode(Node):
    async def prep_async(self,shared): pass
    async def exec_async(self,prep_res): pass
    async def exec_fallback_async(self,prep_res,exc): raise exc
    async def post_async(self,shared,prep_res,exec_res): pass
    async def _exec(self,prep_res):
        for self.cur_retry in range(self.max_retries):
            try: return await self.exec_async(prep_res)
            except Exception as e:
                if self.cur_retry==self.max_retries-1: return await self.exec_fallback_async(prep_res,e)
                if self.wait>0: await asyncio.sleep(self.wait)
    async def run_async(self,shared):
        if self.successors: warnings.warn("Node won't run successors. Use AsyncFlow.")
        return await self._run_async(shared)
    async def _run_async(self,shared): p=await self.prep_async(shared); e=await self._exec(p); return await self.post_async(shared,p,e)
    def _run(self,shared): raise RuntimeError("Use run_async.")

class AsyncBatchNode(AsyncNode,BatchNode):
    async def _exec(self,items): return [await super(AsyncBatchNode,self)._exec(i) for i in items]

class AsyncParallelBatchNode(AsyncNode,BatchNode):
    async def _exec(self,items): return await asyncio.gather(*(super(AsyncParallelBatchNode,self)._exec(i) for i in items))

class AsyncFlow(Flow,AsyncNode):
    async def _orch_async(self,shared,params=None):
        curr,p,last_action =copy.copy(self.start_node),(params or {**self.params}),None
        while curr: curr.set_params(p); last_action=await curr._run_async(shared) if isinstance(curr,AsyncNode) else curr._run(shared); curr=copy.copy(self.get_next_node(curr,last_action))
        return last_action
    async def _run_async(self,shared): p=await self.prep_async(shared); o=await self._orch_async(shared); return await self.post_async(shared,p,o)
    async def post_async(self,shared,prep_res,exec_res): return exec_res

class AsyncBatchFlow(AsyncFlow,BatchFlow):
    async def _run_async(self,shared):
        pr=await self.prep_async(shared) or []
        for bp in pr: await self._orch_async(shared,{**self.params,**bp})
        return await self.post_async(shared,pr,None)

class AsyncParallelBatchFlow(AsyncFlow,BatchFlow):
    async def _run_async(self,shared):
        pr=await self.prep_async(shared) or []
        await asyncio.gather(*(self._orch_async(shared,{**self.params,**bp}) for bp in pr))
        return await self.post_async(shared,pr,None)


================================================
FILE: tabletopmagnat/pocketflow/__init__.pyi.py
================================================
import asyncio
from typing import Any, Dict, List, Optional, Union, TypeVar, Generic

from tabletopmagnat.state.private_state import PrivateState

# Type variables for better type relationships
_PrepResult = TypeVar("_PrepResult")
_ExecResult = TypeVar("_ExecResult")
_PostResult = TypeVar("_PostResult")

# More specific parameter types
ParamValue = Union[str, int, float, bool, None, List[Any], Dict[str, Any]]
SharedData = PrivateState
Params = Dict[str, ParamValue]


class BaseNode(Generic[_PrepResult, _ExecResult, _PostResult]):
    params: Params
    successors: Dict[str, BaseNode[Any, Any, Any]]

    def __init__(self) -> None: ...
    def set_params(self, params: Params) -> None: ...
    def next(
        self, node: BaseNode[Any, Any, Any], action: str = "default"
    ) -> BaseNode[Any, Any, Any]: ...
    def prep(self, shared: SharedData) -> _PrepResult: ...
    def exec(self, prep_res: _PrepResult) -> _ExecResult: ...
    def post(
        self, shared: SharedData, prep_res: _PrepResult, exec_res: _ExecResult
    ) -> _PostResult: ...
    def _exec(self, prep_res: _PrepResult) -> _ExecResult: ...
    def _run(self, shared: SharedData) -> _PostResult: ...
    def run(self, shared: SharedData) -> _PostResult: ...
    def __rshift__(self, other: BaseNode[Any, Any, Any]) -> BaseNode[Any, Any, Any]: ...
    def __sub__(self, action: str) -> _ConditionalTransition: ...


class _ConditionalTransition:
    src: BaseNode[Any, Any, Any]
    action: str

    def __init__(self, src: BaseNode[Any, Any, Any], action: str) -> None: ...
    def __rshift__(self, tgt: BaseNode[Any, Any, Any]) -> BaseNode[Any, Any, Any]: ...


class Node(BaseNode[_PrepResult, _ExecResult, _PostResult]):
    max_retries: int
    wait: Union[int, float]
    cur_retry: int

    def __init__(self, max_retries: int = 1, wait: Union[int, float] = 0) -> None: ...
    def exec_fallback(self, prep_res: _PrepResult, exc: Exception) -> _ExecResult: ...
    def _exec(self, prep_res: _PrepResult) -> _ExecResult: ...


class BatchNode(Node[Optional[List[_PrepResult]], List[_ExecResult], _PostResult]):
    def _exec(self, items: Optional[List[_PrepResult]]) -> List[_ExecResult]: ...


class Flow(BaseNode[_PrepResult, Any, _PostResult]):
    start_node: Optional[BaseNode[Any, Any, Any]]

    def __init__(self, start: Optional[BaseNode[Any, Any, Any]] = None) -> None: ...
    def start(self, start: BaseNode[Any, Any, Any]) -> BaseNode[Any, Any, Any]: ...
    def get_next_node(
        self, curr: BaseNode[Any, Any, Any], action: Optional[str]
    ) -> Optional[BaseNode[Any, Any, Any]]: ...
    def _orch(self, shared: SharedData, params: Optional[Params] = None) -> Any: ...
    def _run(self, shared: SharedData) -> _PostResult: ...
    def post(
        self, shared: SharedData, prep_res: _PrepResult, exec_res: Any
    ) -> _PostResult: ...


class BatchFlow(Flow[Optional[List[Params]], Any, _PostResult]):
    def _run(self, shared: SharedData) -> _PostResult: ...


class AsyncNode(Node[_PrepResult, _ExecResult, _PostResult]):
    async def prep_async(self, shared: SharedData) -> _PrepResult: ...
    async def exec_async(self, prep_res: _PrepResult) -> _ExecResult: ...
    async def exec_fallback_async(
        self, prep_res: _PrepResult, exc: Exception
    ) -> _ExecResult: ...
    async def post_async(
        self, shared: SharedData, prep_res: _PrepResult, exec_res: _ExecResult
    ) -> _PostResult: ...
    async def _exec(self, prep_res: _PrepResult) -> _ExecResult: ...
    async def run_async(self, shared: SharedData) -> _PostResult: ...
    async def _run_async(self, shared: SharedData) -> _PostResult: ...
    def _run(self, shared: SharedData) -> _PostResult: ...


class AsyncBatchNode(
    AsyncNode[Optional[List[_PrepResult]], List[_ExecResult], _PostResult],
    BatchNode[Optional[List[_PrepResult]], List[_ExecResult], _PostResult],
):
    async def _exec(self, items: Optional[List[_PrepResult]]) -> List[_ExecResult]: ...


class AsyncParallelBatchNode(
    AsyncNode[Optional[List[_PrepResult]], List[_ExecResult], _PostResult],
    BatchNode[Optional[List[_PrepResult]], List[_ExecResult], _PostResult],
):
    async def _exec(self, items: Optional[List[_PrepResult]]) -> List[_ExecResult]: ...


class AsyncFlow(
    Flow[_PrepResult, Any, _PostResult], AsyncNode[_PrepResult, Any, _PostResult]
):
    async def _orch_async(
        self, shared: SharedData, params: Optional[Params] = None
    ) -> Any: ...
    async def _run_async(self, shared: SharedData) -> _PostResult: ...
    async def post_async(
        self, shared: SharedData, prep_res: _PrepResult, exec_res: Any
    ) -> _PostResult: ...


class AsyncBatchFlow(
    AsyncFlow[Optional[List[Params]], Any, _PostResult],
    BatchFlow[Optional[List[Params]], Any, _PostResult],
):
    async def _run_async(self, shared: SharedData) -> _PostResult: ...


class AsyncParallelBatchFlow(
    AsyncFlow[Optional[List[Params]], Any, _PostResult],
    BatchFlow[Optional[List[Params]], Any, _PostResult],
):
    async def _run_async(self, shared: SharedData) -> _PostResult: ...



================================================
FILE: tabletopmagnat/services/__init__.py
================================================
[Empty file]


================================================
FILE: tabletopmagnat/services/openai_service.py
================================================
from copy import deepcopy
from typing import Any

from langfuse.openai import AsyncOpenAI
from openai.types.chat import (
    ChatCompletion,
)

from openai._types import Omit

from tabletopmagnat.config.openai_config import OpenAIConfig
from tabletopmagnat.types.dialog import Dialog
from tabletopmagnat.types.messages import AiMessage
from tabletopmagnat.types.messages.tool_message import ToolMessage
from tabletopmagnat.types.tool.openai_tool_params import OpenAIToolParams


class OpenAIService:
    def __init__(self, model_name: str, model_config: OpenAIConfig) -> None:
        self.tools: list[OpenAIToolParams] = []
        self.config = model_config
        self.model: str = model_name
        self.client = AsyncOpenAI(
            api_key=model_config.api_key,
            base_url=model_config.base_url,
        )
        self.structure = None

    def __deepcopy__(self, memo: dict[int, object] | None = None) -> object:
        new_instance = self.__class__(self.model, self.config)
        memo[id(self)] = new_instance
        return new_instance

    def add_mcp_tool(self, tool: OpenAIToolParams) -> None:
        if tool not in self.tools:
            self.tools.append(tool)

    def add_mcp_tools(self, tools: list[OpenAIToolParams]) -> None:
        tmp_tools = [tool for tool in tools if tool not in self.tools]
        self.tools.extend(tmp_tools)

    def bind_structured(self, structure: Any) -> None:
        self.structure = structure

    async def generate(self, dialog: Dialog) -> AiMessage:
        openai_tools = [tool.model_dump(by_alias=True) for tool in self.tools]

        response: ChatCompletion | None = None
        metadata = None
        content = ""

        if self.structure:
            response = await self.client.chat.completions.parse(
                messages=dialog.to_list(),
                model=self.model,
                response_format=self.structure,
            )
            metadata = response.choices[0].message.parsed
            metadata = metadata.model_dump() if metadata else None
        else:
            response: ChatCompletion = await self.client.chat.completions.create(
                messages=dialog.to_list(),
                model=self.model,
                tools=Omit() if not openai_tools else openai_tools,
            )

            content = response.choices[0].message.content
            content = "" if content is None else content.strip()

        tools_openai = response.choices[0].message.tool_calls
        tools = (
            [
                ToolMessage(
                    name=tool.function.name,
                    tool_call_id=tool.id,
                    content=tool.function.arguments,
                )
                for tool in tools_openai
            ]
            if tools_openai
            else []
        )

        # TODO: How will be better to handle this?
        response_msg = AiMessage(
            content=content,
            tool_calls=tools_openai,
            internal_tools=tools,
            metadata=metadata,
        )

        return response_msg



================================================
FILE: tabletopmagnat/state/__init__.py
================================================
[Empty file]


================================================
FILE: tabletopmagnat/state/expert_state.py
================================================
from dataclasses import dataclass
from tabletopmagnat.pocketflow import AsyncFlow


@dataclass(slots=True)
class ExpertState:
    expert_1: AsyncFlow
    expert_2: AsyncFlow
    expert_3: AsyncFlow

    def to_list(self):
        return [self.expert_1, self.expert_2, self.expert_3]



================================================
FILE: tabletopmagnat/state/private_state.py
================================================
from pydantic import BaseModel, Field

from tabletopmagnat.types.dialog import Dialog


class PrivateState(BaseModel):
    dialog: Dialog = Field(default_factory=Dialog)
    expert_1: Dialog = Field(default_factory=Dialog)
    expert_2: Dialog = Field(default_factory=Dialog)
    expert_3: Dialog = Field(default_factory=Dialog)
    summary: Dialog = Field(default_factory=Dialog)



================================================
FILE: tabletopmagnat/structured_output/__init__.py
================================================
[Empty file]


================================================
FILE: tabletopmagnat/structured_output/security.py
================================================
from typing import Literal

from pydantic import BaseModel, Field


class SecurityOutput(BaseModel):
    verdict: Literal["safe", "unsafe"] = Field(..., description="The verdict of the security check")
    user_input: str = Field(..., description="The user input that was checked")
    description: str = Field(..., description="The description of the security check")



================================================
FILE: tabletopmagnat/structured_output/task_classifier.py
================================================
from typing import Literal

from pydantic import BaseModel, Field


class TaskClassifierOutput(BaseModel):
    task: Literal["web_search", "explanation", "clarification"] = Field(
        description="The task of the user. "
        "search -- search for tabletop information in the web. Used only if information is not found in the game rules. "
        "explanation -- explain the **FULL** rules of the tabletop game. Explanation will be from begin to end. "
        "clarification -- ask for clarification on the rules of the tabletop game. Answer will be contains the 1-2 sentence. "
    )



================================================
FILE: tabletopmagnat/structured_output/task_splitter.py
================================================
from pydantic import BaseModel,Field

class TaskSplitterOutput(BaseModel):
    task_for_expert1: str = Field(..., description="Task for expert 1")
    task_for_expert2: str = Field(..., description="Task for expert 2")
    task_for_expert3: str = Field(..., description="Task for expert 3")


================================================
FILE: tabletopmagnat/subgraphs/__init__.py
================================================
[Empty file]


================================================
FILE: tabletopmagnat/subgraphs/rags.py
================================================
from copy import deepcopy
from typing import Callable

from blacksheep.server.controllers import abstract
from tabletopmagnat.node.abstract_node import AbstractNode
from tabletopmagnat.node.llm_node import LLMNode
from tabletopmagnat.node.mcp_tool_node import MCPToolNode
from tabletopmagnat.pocketflow import AsyncFlow, AsyncNode
from tabletopmagnat.services.openai_service import OpenAIService
from tabletopmagnat.types.dialog import Dialog
from tabletopmagnat.types.tool.mcp import MCPTools
from tabletopmagnat.types.tool.openai_tool_params import OpenAIToolParams
from tabletopmagnat.state.private_state import PrivateState


class RASG:
    @staticmethod
    async def create_subgraph(
        name: str,
        prompt_name: str,
        openai_service: OpenAIService,
        mcp_tools: MCPTools,
        dialog_selector: Callable[[PrivateState], Dialog],
    ):
        tools: list[OpenAIToolParams] = await mcp_tools.get_openai_tools()

        # Create universal node and bind tools to them
        universal_node = LLMNode(
            name=f"{name}_universal_node",
            prompt_name=prompt_name,
            dialog_selector=dialog_selector,
            llm_service=deepcopy(openai_service),
            max_retries=3,
            wait=2,
        )
        universal_node.bind_tools(tools)

        # Create tool nodes
        tool_node = MCPToolNode(
            name=f"{name}_tool_node",
            mcp_tool=mcp_tools,
            dialog_selector=dialog_selector,
        )

        abstract_node = AsyncNode()

        # Connect
        universal_node - "tools" >> tool_node
        universal_node - "default" >> abstract_node
        tool_node >> universal_node

        # Create flow
        flow = AsyncFlow(start=universal_node)

        return flow



================================================
FILE: tabletopmagnat/types/__init__.py
================================================
[Empty file]


================================================
FILE: tabletopmagnat/types/dialog/__init__.py
================================================
"""
Dialog Package Initialization.

This module initializes the `dialog` package and re-exports the main `Dialog` class.
The `Dialog` class is used to manage sequences of messages in the TabletopMagnat application.

Exports:
    Dialog: A class for managing and organizing a conversation history as a list of message objects.
"""

from tabletopmagnat.types.dialog.dialog import Dialog

__all__ = ["Dialog"]


================================================
FILE: tabletopmagnat/types/dialog/dialog.py
================================================
"""
Dialog Class Module.

This module provides a `Dialog` class to manage and organize a sequence of messages
in the TabletopMagnat application. It allows for adding messages and converting them
into a list of dictionaries, suitable for external processing or API interaction.

Classes:
    Dialog: A container for managing a sequence of message objects.
"""
from pydantic import BaseModel

from tabletopmagnat.types.messages.base_message import BaseMessage


class Dialog(BaseModel):
    """
    A class representing a sequence of messages in a conversation.

    Attributes:
        messages (list[BaseMessage]): A list of message objects in the dialog.

    Methods:
        add_message(message): Adds a message to the dialog if it is an instance of BaseMessage.
        to_list(): Converts all messages in the dialog into a list of dictionaries.
    """
    messages: list[BaseMessage] | None = None

    def add_message(self, message: BaseMessage) -> None:
        """
        Adds a message to the dialog.

        Args:
            message (BaseMessage): The message to be added.

        Raises:
            TypeError: If the provided message is not an instance of BaseMessage.
        """
        if self.messages is None:
            self.messages = []

        if isinstance(message, BaseMessage):
            self.messages.append(message)
        else:
            raise TypeError("Message must be an instance of BaseMessage")

    def to_list(self) -> list[dict]:
        """
        Converts each message in the dialog into a dictionary format.

        Returns:
            list[dict]: A list of dictionaries, each representing a message with 'role' and 'content' keys.
        """
        return [message.to_dict() for message in self.messages] if self.messages else []

    def __iadd__(self, other):
        if isinstance(other, Dialog):
            self.messages.extend(other.messages)
            return self


        raise TypeError(f"Cannot add {type(other)} to Dialog")

    def get_last_message(self):
        return self.messages[-1] if self.messages and len(self.messages) else None

    def pop_last_message(self):
        return self.messages.pop() if self.messages and len(self.messages) else None

    def replace_last_message(self, message: BaseMessage):
        if self.messages:
            self.messages[-1] = message
        else:
            self.messages = [message]

        return None


================================================
FILE: tabletopmagnat/types/messages/__init__.py
================================================
"""
Messages Package Initialization.

This module initializes the `messages` package and re-exports key classes and enums
used for representing different types of messages in the TabletopMagnat application.
It provides a unified interface to access message-related components.

Exports:
    BaseMessage: Abstract base class for all message types.
    MessageRoles: Enum defining roles for message senders (e.g., user, assistant, system).
    AiMessage: Class representing messages generated by the AI/assistant.
    UserMessage: Class representing messages sent by the user.
    SystemMessage: Class representing system-level or internal messages.
"""

from tabletopmagnat.types.messages.base_message import BaseMessage
from tabletopmagnat.types.messages.message_roles import MessageRoles
from tabletopmagnat.types.messages.ai_messages import AiMessage
from tabletopmagnat.types.messages.user_messages import UserMessage
from tabletopmagnat.types.messages.system_messages import SystemMessage
from tabletopmagnat.types.messages.developer_messages import DeveloperMessage

__all__ = ["BaseMessage", "MessageRoles", "AiMessage", "UserMessage", "SystemMessage", "DeveloperMessage"]


================================================
FILE: tabletopmagnat/types/messages/ai_messages.py
================================================
"""
Message Types Module.

This module provides classes representing different message types used in the TabletopMagnat application.
Each class inherits from `BaseMessage` and defines a specific role (User, Assistant, System).
The classes also include a method to convert the message to a dictionary format suitable for external APIs.

Classes:
    UserMessage: Represents a message sent by the user.
    AssistantMessage: Represents a message generated by the assistant.
    SystemMessage: Represents a system-level message used for internal instructions or context.
"""

from typing import override

from openai.types.chat import ChatCompletionMessageFunctionToolCall
from pydantic import Field

from tabletopmagnat.types.messages.base_message import BaseMessage
from tabletopmagnat.types.messages.message_roles import MessageRoles
from tabletopmagnat.types.messages.tool_message import ToolMessage


class AiMessage(BaseMessage):
    """
    A message class for assistant responses.

    Attributes:
        role (MessageRoles): The role of the message sender, fixed to `MessageRoles.ASSISTANT`.

    Methods:
        to_dict(): Converts the message into a dictionary with 'role' and 'content' keys.
    """

    role: MessageRoles = MessageRoles.ASSISTANT
    tool_calls: list[ChatCompletionMessageFunctionToolCall] | None = Field(default=None)
    internal_tools: list[ToolMessage] = Field(exclude=True, default_factory=list)

    @override
    def to_dict(self):
        """
        Converts the message into a dictionary format.

        Returns:
            dict: A dictionary with 'role' and 'content' keys.
        """
        return {"role": str(self.role.value), "content": self.content, "tool_calls": self.tool_calls}



================================================
FILE: tabletopmagnat/types/messages/base_message.py
================================================
"""
Base Message Class Module.

This module defines the abstract base class for all message types in the TabletopMagnat application.
It serves as a template for user, assistant, and system messages, ensuring a consistent interface
and structure across different message implementations.

Classes:
    BaseMessage: Abstract base class that provides the foundation for message objects.
"""

from abc import ABC, abstractmethod

from pydantic import BaseModel, Field


class BaseMessage(BaseModel, ABC):
    """
    Abstract base class for message objects.

    Attributes:
        content (str): The main content or text of the message.

    Methods:
        to_dict(): Abstract method that must be implemented by subclasses to return
                   a dictionary representation of the message.
    """

    content: str
    metadata: dict | None = Field(default=None, exclude=True)

    @abstractmethod
    def to_dict(self):
        """
        Converts the message into a dictionary format.

        This is an abstract method and must be implemented by subclasses.

        Returns:
            dict: A dictionary representation of the message with 'role' and 'content' keys.
        """
        raise NotImplementedError()



================================================
FILE: tabletopmagnat/types/messages/developer_messages.py
================================================
"""
Message Types Module.

This module provides classes representing different message types used in the TabletopMagnat application.
Each class inherits from `BaseMessage` and defines a specific role (User, Assistant, System).
The classes also include a method to convert the message to a dictionary format suitable for external APIs.

**Note**: The `Developer` message type is used exclusively in the GPT-OSS version of the application for internal developer-specific instructions or context.
It is not part of the standard message schema and is ignored or unsupported in other versions.

Classes:
    Developer: Represents a developer-level message used for internal instructions or context in GPT-OSS.
"""
from typing import override

from tabletopmagnat.types.messages.base_message import BaseMessage
from tabletopmagnat.types.messages.message_roles import MessageRoles


class DeveloperMessage(BaseMessage):
    """
    A message class for developer-level instructions or context in GPT-OSS.

    This message type is only recognized and processed in the GPT-OSS version of the application.
    It allows sending internal developer-specific directives that influence behavior during development or debugging.
    In production or other versions, this role may be ignored or treated as an unknown role.

    Attributes:
        role (MessageRoles): The role of the message sender, fixed to `MessageRoles.DEVELOPER`.

    Methods:
        to_dict(): Converts the message into a dictionary with 'role' and 'content' keys.
    """

    role: MessageRoles = MessageRoles.DEVELOPER

    @override
    def to_dict(self):
        """
        Converts the message into a dictionary format.

        Returns:
            dict: A dictionary with 'role' and 'content' keys, where role is stringified using its value.
        """
        return {"role": str(self.role.value), "content": self.content}


================================================
FILE: tabletopmagnat/types/messages/message_roles.py
================================================
from enum import StrEnum


class MessageRoles(StrEnum):
    ASSISTANT = "assistant"
    USER = "user"
    SYSTEM = "system"
    TOOL = "tool"
    DEVELOPER = "developer"



================================================
FILE: tabletopmagnat/types/messages/system_messages.py
================================================
"""
Message Types Module.

This module provides classes representing different message types used in the TabletopMagnat application.
Each class inherits from `BaseMessage` and defines a specific role (User, Assistant, System).
The classes also include a method to convert the message to a dictionary format suitable for external APIs.

Classes:
    UserMessage: Represents a message sent by the user.
    AssistantMessage: Represents a message generated by the assistant.
    SystemMessage: Represents a system-level message used for internal instructions or context.
"""
from typing import override

from tabletopmagnat.types.messages.base_message import BaseMessage
from tabletopmagnat.types.messages.message_roles import MessageRoles


class SystemMessage(BaseMessage):
    """
    A message class for system-level instructions or context.

    Attributes:
        role (MessageRoles): The role of the message sender, fixed to `MessageRoles.SYSTEM`.

    Methods:
        to_dict(): Converts the message into a dictionary with 'role' and 'content' keys.
    """

    role: MessageRoles = MessageRoles.SYSTEM

    @override
    def to_dict(self):
        """
        Converts the message into a dictionary format.

        Returns:
            dict: A dictionary with 'role' and 'content' keys.
        """
        return {"role": str(self.role.value), "content": self.content}



================================================
FILE: tabletopmagnat/types/messages/tool_message.py
================================================
"""
Message Types Module.

This module provides classes representing different message types used in the TabletopMagnat application.
Each class inherits from `BaseMessage` and defines a specific role (User, Assistant, System).
The classes also include a method to convert the message to a dictionary format suitable for external APIs.

Classes:
    UserMessage: Represents a message sent by the user.
    AssistantMessage: Represents a message generated by the assistant.
    SystemMessage: Represents a system-level message used for internal instructions or context.
"""

from typing import override

from tabletopmagnat.types.messages.base_message import BaseMessage
from tabletopmagnat.types.messages.message_roles import MessageRoles


class ToolMessage(BaseMessage):
    """
    A message class for system-level instructions or context.

    Attributes:
        role (MessageRoles): The role of the message sender, fixed to `MessageRoles.SYSTEM`.

    Methods:
        to_dict(): Converts the message into a dictionary with 'role' and 'content' keys.
    """

    name: str = "function_name"
    tool_call_id: str = "tool_call_id"
    role: MessageRoles = MessageRoles.TOOL

    @override
    def to_dict(self):
        """
        Converts the message into a dictionary format.

        Returns:
            dict: A dictionary with 'role' and 'content' keys.
        """
        return {
            "role": str(self.role.value),
            "content": self.content,
            "name": self.name,
            "tool_call_id": self.tool_call_id,
        }



================================================
FILE: tabletopmagnat/types/messages/user_messages.py
================================================
"""
Message Types Module.

This module provides classes representing different message types used in the TabletopMagnat application.
Each class inherits from `BaseMessage` and defines a specific role (User, Assistant, System).
The classes also include a method to convert the message to a dictionary format suitable for external APIs.

Classes:
    UserMessage: Represents a message sent by the user.
    AssistantMessage: Represents a message generated by the assistant.
    SystemMessage: Represents a system-level message used for internal instructions or context.
"""

from typing import override

from tabletopmagnat.types.messages.base_message import BaseMessage
from tabletopmagnat.types.messages.message_roles import MessageRoles


class UserMessage(BaseMessage):
    """
    A message class for user input.

    Attributes:
        role (MessageRoles): The role of the message sender, fixed to `MessageRoles.USER`.

    Methods:
        to_dict(): Converts the message into a dictionary with 'role' and 'content' keys.
    """

    role: MessageRoles = MessageRoles.USER

    @override
    def to_dict(self):
        """
        Converts the message into a dictionary format.

        Returns:
            dict: A dictionary with 'role' and 'content' keys.
        """
        return {"role": str(self.role.value), "content": self.content}



================================================
FILE: tabletopmagnat/types/tool/__init__.py
================================================
from tabletopmagnat.types.tool.tool_header import ToolHeader

__all__ = ["ToolHeader"]


================================================
FILE: tabletopmagnat/types/tool/openai_tool_params.py
================================================
from pydantic import BaseModel, Field

class FunctionParams(BaseModel):
    name: str
    description: str
    parameters: dict

class OpenAIToolParams(BaseModel):
    fn_type: str = Field(default="function", alias="type")
    fn_params: FunctionParams = Field(alias="function")




================================================
FILE: tabletopmagnat/types/tool/tool_header.py
================================================
from pydantic import BaseModel, Field

class ToolHeader(BaseModel):
    authorization: str = Field(default="empty_token", alias="Authorization")

    def set_auth(self, token: str):
        self.authorization = f"Bearer {token}"


================================================
FILE: tabletopmagnat/types/tool/mcp/__init__.py
================================================
from tabletopmagnat.types.tool.mcp.mcp_server import MCPServer
from tabletopmagnat.types.tool.mcp.mcp_servers import MCPServers
from tabletopmagnat.types.tool.mcp.mcp_tool import MCPTools

__all__ = ["MCPServer", "MCPServers", "MCPTools"]


================================================
FILE: tabletopmagnat/types/tool/mcp/mcp_server.py
================================================
from typing import Literal

from pydantic import BaseModel

from tabletopmagnat.types.tool import ToolHeader


class MCPServer(BaseModel):
    transport: Literal["http", "sse"]
    url: str
    headers: ToolHeader
    auth: Literal["oauth", "bearer"]



================================================
FILE: tabletopmagnat/types/tool/mcp/mcp_servers.py
================================================
from pydantic import BaseModel, Field

from tabletopmagnat.types.tool.mcp.mcp_server import MCPServer


class MCPServers(BaseModel):
    mcp_server: dict[str, MCPServer] = Field(default_factory=dict, alias="mcpServers")



================================================
FILE: tabletopmagnat/types/tool/mcp/mcp_tool.py
================================================
import json

from fastmcp import Client
from fastmcp.client.client import CallToolResult

from tabletopmagnat.types.tool.mcp import MCPServers
from tabletopmagnat.types.tool.openai_tool_params import (
    FunctionParams,
    OpenAIToolParams,
)


class MCPTools:
    def __init__(self, mcp_servers: MCPServers):
        config = mcp_servers.model_dump(by_alias=True)
        self._client: Client = Client(config)
        self._tools_name: list[str | OpenAIToolParams] = []

    def get_client(self):
        return self._client

    async def get_tool_list(self):
        async with self._client:
            tools = await self._client.list_tools()
            self._tools_name = [tool.name for tool in tools]
            return tools

    async def get_openai_tools(self):
        async with self._client:
            tools = await self._client.list_tools()
            tools_json = [
                OpenAIToolParams(
                    function=FunctionParams(
                        name=tool.name,
                        description=tool.description,
                        parameters=tool.inputSchema,
                    )
                )
                for tool in tools
            ]

            self._tools_name = [tool.name for tool in tools]
            return tools_json

    async def call_tool(self, tool_name: str, tool_input: dict | str) -> CallToolResult:
        tools = json.loads(tool_input) if isinstance(tool_input, str) else tool_input

        async with self._client:
            if tool_name in self._tools_name:
                return await self._client.call_tool(tool_name, tools)

            raise ValueError(f"Tool {tool_name} not found")


